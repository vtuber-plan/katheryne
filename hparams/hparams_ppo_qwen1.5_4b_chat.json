{
    "train_stage": "ppo",
    "conv_format": "qwen",
    "end_of_conversation": 151643,
    "data_path": [
        {
            "path": "Dahoas/rm-static",
            "sample": 1.0,
            "preprocessor": "rlhf_rm_static"
        }
    ],
    "data_output_path": "./tmp/data_files/",
    "model_path": "Qwen/Qwen1.5-4B-chat",
    "tokenizer_path": "Qwen/Qwen1.5-4B-chat",
    "model_class": "AutoModelForSequenceClassification",
    "atten_class": "eager",
    "reward_model_path": "llm_trainer_reward/lightning_logs/version_0/huggingface_format/checkpoint-step-1100",
    "reward_tokenizer_path": "Qwen/Qwen1.5-4B-chat",
    "per_device_train_mini_batch_size": 2,
    "per_device_train_batch_size": 32,
    "per_device_eval_mini_batch_size": 8,
    "accumulate_grad_batches": 16,
    "max_seq_len": 2048,
    "checkpoint_every_n_train_steps": 100,
    "log_every_n_steps": 1,
    "val_check_interval": 0.25,
    "limit_val_batches": 0.1,
    "learning_rate": 4e-5,
    "betas": [0.9, 0.95],
    "eps": 8e-6,
    "lr_decay": 0.999875,
    "lr_scheduler_type": "cosine",
    "num_warmup_steps": 100,
    "max_epochs": 300,
    "disable_dropout": true,
    "model_torch_dtype": "auto",
    "bf16": true,
    "gradient_checkpointing": true,
    "weight_decay": 0.0,
    "gradient_clip_algorithm": "norm",
    "gradient_clip_val": 1.0
}
