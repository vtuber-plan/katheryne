{
    "data_path": [
        "bigscience-data/roots_zh-cn_wikipedia"
    ],
    "data_output_path": "./tmp/data_files/",
    "model_name_or_path": "meta-llama/Llama-2-7b-hf",
    "per_device_train_batch_size": 2,
    "per_device_eval_batch_size": 4,
    "accumulate_grad_batches": 64,
    "max_seq_len": 1024,
    "checkpoint_every_n_train_steps": 1000,
    "log_every_n_steps": 1,
    "val_check_interval": 0.25,
    "limit_val_batches": 0.1,
    "learning_rate": 2e-5,
    "betas": [0.9, 0.95],
    "eps": 1e-6,
    "lr_decay": 0.999875,
    "lr_scheduler_type": "cosine",
    "num_warmup_steps": 100,
    "max_epochs": 300,
    "disable_dropout": true,
    "model_torch_dtype": "auto",
    "bf16": true,
    "gradient_checkpointing": false,
    "weight_decay": 0.0,
    "strategy": "ddp",
    "strategy_params": {
        "find_unused_parameters": false
    },
    "lora": {
        "r": 128,
        "target_modules": ["q_proj", "v_proj"]
    }
}
